---
title: "Client Report - Project 4"
subtitle: "Course DS 250"
author: "Dale McBride"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import numpy as np
import plotly.express as px
```

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import tree
from sklearn import metrics
```


## Elevator pitch

_The analysis reveals compelling insights into the correlation between property attributes and construction years, indicating a noticeable rise in livable space density and net prices of homes after 1980. Report 2 emerges as the most robust classifier, showcasing superior predictive capability with high accuracy, precision, recall, and balanced F1-scores. Key features such as architectural style, garage type, quality rating, and living area size play crucial roles in predicting property outcomes, highlighting their significance in characterizing properties and influencing classification decisions. The model's ability to maintain a strong balance between precision and recall underscores its reliability in making precise predictions for different property classes, providing valuable insights for stakeholders in the real estate industry._


```{python}
#| label: project data
#| code-summary: Read and format project data
# Include and execute your code here
df = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv")
```


## QUESTION|TASK 1

__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__

_After 1980, there is a noticeable rise in both the livable space density and the net price of homes, suggesting a possible correlation between the construction year and the size and cost of the properties. The first scatter plot shows a corrolation between the liveable area and the year that it was built. The number of houses larger than 5000 square feet built after 1980 significantly increases. Showing a corrolation between house size and the year built. The second chart compares the net price and the year that it is built. Showing that if the netprice is higher than it is likely that the house was built after 1980._


```{python}
#| label: Q1 chart 1
#| code-summary: plot example
#| fig-align: center
# Include and execute your code here
subset = df.filter(['before1980', 'yrbuilt', 'netprice', 'smonth', 'livearea', 'deduct', 'stories', 'livearea',])

subset['color'] = subset['before1980'].apply(lambda x: 'Before 1980' if x else 'After 1980')

color_map_labels = {'Before 1980': 'blue', 'After 1980': 'red'}

chart = px.scatter(subset,
    title='Liveable Area versus house build year',
    x="yrbuilt", 
    y="livearea",
    color="color",
    color_discrete_map=color_map_labels
)

chart.update_layout(
    xaxis_title="Year Built",
    yaxis_title="Liveable Area"
)

chart.show()
```

_The chart shows a siginificant increase in quantity of homes with higher amounts of liveable area in them after 1980._

```{python}
#| label: Q1 chart 2
#| code-summary: plot example
#| fig-align: center
# Include and execute your code here

subset = df.filter(['before1980', 'yrbuilt', 'netprice', 'numbdrm', 'numbaths', 'nocars'])

subset['color'] = subset['before1980'].apply(lambda x: 'Before 1980' if x else 'After 1980')

color_map_labels = {'Before 1980': 'blue', 'After 1980': 'red'}

chart = px.scatter(subset,
    title="Comparing Net price and Build Year",
    x="yrbuilt", 
    y="netprice",
    color="color",
    color_discrete_map=color_map_labels

)

chart.update_layout(
    xaxis_title="Year Built",
    yaxis_title="Net Price"
)

chart.show()
```

_This chart also shows an increase in the quantity of more expensive homes after the year of 1980. There are a few outliers in the data prior to 1980, but not enough to change the data._

## QUESTION|TASK 2

__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__

_Report 2 exhibits the highest performance across all evaluation metrics, boasting the highest accuracy, precision, recall, and F1-score among the three models evaluated. With precision and recall scores of 0.90 and 0.94 for Class 0 and Class 1 respectively, Report 2 demonstrates its effectiveness in correctly identifying both positive and negative instances. The balanced F1-score of 0.90 and 0.94 for Class 0 and Class 1 further confirms the model's ability to maintain a trade-off between precision and recall. Overall, Report 2 emerges as the most robust classifier, showcasing superior predictive capability on the dataset._

```{python}
#| label: Q2
#| code-summary: Read and format data
# Include and execute your code here

X_pred = df.drop(df.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)

y_pred = df.filter(regex = "before1980")

X_train, X_test, y_train, y_test = train_test_split(
    X_pred, y_pred, test_size = .32, random_state = 78)


```

```{python}
#| label: chart
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
```

```{python}
print(metrics.classification_report(y_test, y_pred))
```

```{python}
#| fig-cap: "This is the best algorithm"
clf = RandomForestClassifier()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
#evaluate the difference between the two
```

```{python}
print(metrics.classification_report(y_test, y_pred))
```

```{python}
clf = GradientBoostingClassifier()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
```

```{python}
print(metrics.classification_report(y_test, y_pred))
```



## QUESTION|TASK 3

__Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.__

_In the classification model, several features play crucial roles in predicting outcomes, with the most significant being the architectural style denoted as "one-story." This feature, with the highest importance value of 0.34, indicates whether a property follows a one-story architectural design, suggesting its role in the model's decision-making process. Additionally, the presence of an attached garage (gartype_Att) emerges as the second most influential feature, emphasizing its importance in determining property classifications. Quality rating (quality_C) and living area size (livearea) also exhibit notable importance values, further contributing to the model's predictive accuracy. The prevalence of these features underscores their significance in characterizing properties and their potential impact on predicted outcomes, providing valuable insights into the factors driving the classification decisions._

```{python}
# feature_importance = clf.feature_importances_
# feature_names = X_train.columns

# # Create a dictionary to associate feature names with their importance values
# feature_importance_dict = dict(zip(feature_names, feature_importance))

# # Sort the dictionary by importance values in descending order
# sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# # Display the top features and their importance values
# for feature, importance in sorted_feature_importance:
#     print(f"Feature: {feature}, Importance: {importance}")
```


```{python}
#| label: Q3
#| code-summary: Read and format data
# Include and execute your code here
df_features = pd.DataFrame(
    {'f_names': X_train.columns, 
    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)

    #what does before 1980 have to do with the top value in the bar chart
```


```{python}
#| label: Q3 feature importance chart!!!!
#| code-summary: plot example
#| fig-align: center
# Include and execute your code here
chart = px.bar(df_features.head(10),
    x='f_values', 
    y='f_names'
)

chart.update_layout(yaxis={'categoryorder':'total ascending'})
```


## QUESTION|TASK 4

__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__

_The classification model displays robust performance, evident through its high precision, recall, and F1-score values across both classes. Precision measures the proportion of true positive predictions out of all positive predictions made by the model, indicating its ability to accurately identify positive instances while minimizing false positives. Recall, on the other hand, measures the model's effectiveness in capturing actual positive instances, with higher values suggesting fewer false negatives. The F1-score, being the mean of precision and recall, provides a balanced assessment of the model's overall accuracy, considering both precision and recall simultaneously. With precision rates of 0.90 for class 0 and 0.94 for class 1, recall rates of 0.90 for class 0 and 0.94 for class 1, and balanced F1-scores of 0.90 for class 0 and 0.94 for class 1, the model demonstrates its reliability in making precise predictions for each class while maintaining a strong balance between precision and recall._